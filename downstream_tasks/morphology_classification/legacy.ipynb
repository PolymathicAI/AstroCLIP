{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from helpers import process, to_pil_image, dr2_rgb\n",
    "from PIL import Image as im\n",
    "from astropy.table import Table, join, vstack\n",
    "from astropy.coordinates import SkyCoord, match_coordinates_sky\n",
    "from astropy import units as u\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    ToTensor,\n",
    "    Normalize,\n",
    "    Resize,\n",
    "    InterpolationMode,\n",
    "    CenterCrop,\n",
    ")\n",
    "\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    os.path.abspath(\n",
    "        \"/mnt/home/lparker/Documents/AstroFoundationModel/AstroDino/dinov2/\"\n",
    "    ),\n",
    ")\n",
    "sys.path.insert(\n",
    "    0, os.path.abspath(\"/mnt/home/lparker/Documents/AstroFoundationModel/AstroDino/\")\n",
    ")\n",
    "from dinov2.utils.config import setup\n",
    "from dinov2.models import build_model_from_cfg\n",
    "from dinov2.fsdp import FSDPCheckpointer\n",
    "from dinov2.train.ssl_meta_arch import SSLMetaArch\n",
    "from dinov2.eval.setup import setup_and_build_model\n",
    "from dinov2.data.transforms import make_normalize_transform\n",
    "\n",
    "# Image Files locations\n",
    "files_north = [\n",
    "    os.path.join(\n",
    "        \"/mnt/ceph/users/polymathic/external_data/astro/DECALS_Stein_et_al/north/\",\n",
    "        \"images_npix152_0%02d000000_0%02d000000.h5\" % (i, i + 1),\n",
    "    )\n",
    "    for i in range(10)\n",
    "]\n",
    "files_south = [\n",
    "    os.path.join(\n",
    "        \"/mnt/ceph/users/polymathic/external_data/astro/DECALS_Stein_et_al/south/\",\n",
    "        \"images_npix152_0%02d000000_0%02d000000.h5\" % (i, i + 1),\n",
    "    )\n",
    "    for i in range(62)\n",
    "]\n",
    "\n",
    "# Classifications location\n",
    "gz5_decals_path = \"/mnt/home/lparker/ceph/gz_decals_volunteers_5.csv\"\n",
    "gz2_sdss_path = \"/mnt/home/lparker/ceph/gz2_hart16.csv\"\n",
    "\n",
    "# Transformations for models\n",
    "MEAN = (0.485, 0.456, 0.406)\n",
    "STD = (0.229, 0.224, 0.225)\n",
    "img_transforms = Compose(\n",
    "    [\n",
    "        to_pil_image,\n",
    "        Resize(152, InterpolationMode.BICUBIC),\n",
    "        ToTensor(),\n",
    "        CenterCrop(144),\n",
    "        Normalize(MEAN, STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class config:\n",
    "    output_dir = \"/mnt/home/lparker/ceph/dino_training\"\n",
    "    config_file = \"../astrodino/configs/ssl_default_config.yaml\"\n",
    "    pretrained_weights = \"/mnt/home/lparker/ceph/astrodino/vitl12_simplified_better_wd/training_199999/teacher_checkpoint.pth\"\n",
    "    opts = []\n",
    "\n",
    "\n",
    "def get_paired_classifications(sky, gz_survey):\n",
    "    if sky == \"south\":\n",
    "        files = files_south\n",
    "    elif sky == \"north\":\n",
    "        files = files_north\n",
    "    else:\n",
    "        raise ValueError(\"Not supported sky type, choose south or north\")\n",
    "\n",
    "    if gz_survey == \"gz2\":\n",
    "        classifications_path = gz2_sdss_path\n",
    "    elif gz_survey == \"gz5\":\n",
    "        classifications_path = gz5_decals_path\n",
    "    else:\n",
    "        raise ValueError(\"Not supported gz_survey type, choose gz2 or gz5\")\n",
    "\n",
    "    print(f\"Sky type is {sky}, survey type is {gz_survey}\", flush=True)\n",
    "\n",
    "    morphologies = Table.read(classifications_path, format=\"ascii\")\n",
    "\n",
    "    ra_list = []\n",
    "    dec_list = []\n",
    "    index_list = []\n",
    "    file_list = []\n",
    "\n",
    "    print(\"Processing files\", flush=True)\n",
    "    for i, file in enumerate(tqdm(files)):\n",
    "        with h5py.File(file, \"r\") as f:\n",
    "            ra = f[\"ra\"][:]\n",
    "            dec = f[\"dec\"][:]\n",
    "\n",
    "            # Append data to lists\n",
    "            ra_list.extend(ra)\n",
    "            dec_list.extend(dec)\n",
    "            file_list.extend([file] * len(ra))\n",
    "            index_list.extend(range(0, len(ra)))\n",
    "\n",
    "    positions = Table(\n",
    "        [ra_list, dec_list, index_list, file_list], names=(\"ra\", \"dec\", \"index\", \"file\")\n",
    "    )\n",
    "\n",
    "    table1 = positions\n",
    "    table2 = morphologies\n",
    "\n",
    "    coords1 = SkyCoord(ra=table1[\"ra\"] * u.degree, dec=table1[\"dec\"] * u.degree)\n",
    "    coords2 = SkyCoord(ra=table2[\"ra\"] * u.degree, dec=table2[\"dec\"] * u.degree)\n",
    "\n",
    "    print(\"Matching coordinates\", flush=True)\n",
    "    idx, d2d, d3d = coords1.match_to_catalog_sky(coords2).to(u.arcsec).value\n",
    "\n",
    "    max_sep = 0.5 * u.arcsec\n",
    "    sep_constraint = d2d < max_sep\n",
    "\n",
    "    classifications = table2[idx[sep_constraint]]\n",
    "    positions_matched = table1[sep_constraint]\n",
    "    classifications[\"index\"] = np.array(positions_matched[\"index\"])\n",
    "    classifications[\"file\"] = np.array(positions_matched[\"file\"])\n",
    "    classifications[\"image\"] = np.zeros((len(classifications), 3, 152, 152))\n",
    "\n",
    "    print(\"Generating catalog with images\", flush=True)\n",
    "    for i, file in enumerate(files):\n",
    "        print(f\"Processing file {i+1}/{len(files)}\", flush=True)\n",
    "        images = []\n",
    "        with h5py.File(file, \"r\") as f:\n",
    "            for k, entry in enumerate(tqdm(classifications)):\n",
    "                if entry[\"file\"] != file:\n",
    "                    continue\n",
    "                index = entry[\"index\"]\n",
    "                classifications[k][\"image\"] = f[\"images\"][index]\n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = get_paired_classifications(\"north\", \"gz5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    \"/mnt/home/lparker/Documents/AstroFoundationModel/AstroCLIP_legacy/notebooks/tutorial/\",\n",
    ")\n",
    "\n",
    "from leopoldo import AstroCLIP, OutputExtractor, seq_decoder, config, forward_im\n",
    "from tutorial_helpers import load_model_from_ckpt, forward\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "import copy\n",
    "\n",
    "\n",
    "class config:\n",
    "    output_dir = \"/mnt/home/lparker/ceph/dino_training\"\n",
    "    config_file = \"/mnt/home/lparker/Documents/AstroFoundationModel/AstroDino_legacy/astrodino/configs/ssl_default_config.yaml\"\n",
    "    pretrained_weights = \"/mnt/home/lparker/ceph/astrodino/vitl12_simplified_better_wd/training_199999/teacher_checkpoint.pth\"\n",
    "    opts = []\n",
    "\n",
    "\n",
    "# Specify transforms\n",
    "MEAN = (0.485, 0.456, 0.406)  # Imagenet default mean\n",
    "STD = (0.229, 0.224, 0.225)  # Imagenet default std\n",
    "img_transforms = Compose([Normalize(MEAN, STD)])\n",
    "\n",
    "# set this\n",
    "embed_dim = 512\n",
    "\n",
    "# Define DINO model\n",
    "img_model, dtype = setup_and_build_model(config())\n",
    "\n",
    "DINO = copy.deepcopy(img_model)\n",
    "\n",
    "# Extract encoder_q from Moco_v2 model\n",
    "img_model.forward = forward_im.__get__(img_model)\n",
    "img_model = OutputExtractor(img_model, embed_dim=embed_dim, freeze_backbone=True)\n",
    "num_params = np.sum(np.fromiter((p.numel() for p in img_model.parameters()), int))\n",
    "print(f\"Number of parameters in image model: {num_params:,}\")\n",
    "\n",
    "# The model is saved in the Seqformer branch of Fi-LLM\n",
    "model_path = \"/mnt/home/sgolkar/ceph/saves/fillm/run-seqformer-2708117\"\n",
    "out = load_model_from_ckpt(model_path)\n",
    "config = out[\"config\"]\n",
    "spec_model = out[\"model\"]\n",
    "spec_model.forward = forward.__get__(spec_model, type(img_model))\n",
    "num_params = np.sum(np.fromiter((p.numel() for p in spec_model.parameters()), int))\n",
    "print(f\"Number of parameters in spectrum model: {num_params:,}\")\n",
    "\n",
    "# Define image and spectrum encoders\n",
    "image_encoder = img_model\n",
    "spectrum_encoder = seq_decoder(\n",
    "    model=spec_model, embed_dim=embed_dim, freeze_backbone=True\n",
    ")\n",
    "\n",
    "# Set up AstroCLIP\n",
    "astroclip = AstroCLIP(image_encoder, spectrum_encoder, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/mnt/home/lparker/Documents/AstroFoundationModel/AstroCLIP_legacy/notebooks/tutorial/astroclip-clip-explore/03x73csv/checkpoints/epoch=14-step=2310.ckpt\"\n",
    "\n",
    "ckpt = torch.load(file)\n",
    "astroclip.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image as im\n",
    "\n",
    "\n",
    "def sdss_rgb(imgs, bands, scales=None, m=0.02):\n",
    "    rgbscales = {\n",
    "        \"u\": (2, 1.5),  # 1.0,\n",
    "        \"g\": (2, 2.5),\n",
    "        \"r\": (1, 1.5),\n",
    "        \"i\": (0, 1.0),\n",
    "        \"z\": (0, 0.4),  # 0.3\n",
    "    }\n",
    "    if scales is not None:\n",
    "        rgbscales.update(scales)\n",
    "\n",
    "    I = 0\n",
    "    for img, band in zip(imgs, bands):\n",
    "        plane, scale = rgbscales[band]\n",
    "        img = torch.maximum(torch.tensor(0), img * scale + m)\n",
    "        I = I + img\n",
    "    I /= len(bands)\n",
    "    Q = 20\n",
    "    fI = torch.arcsinh(Q * I) / torch.sqrt(torch.tensor(Q))\n",
    "    I += (I == 0.0) * 1e-6\n",
    "    H, W = I.shape\n",
    "    rgb = torch.zeros((H, W, 3)).to(torch.float32)\n",
    "    for img, band in zip(imgs, bands):\n",
    "        plane, scale = rgbscales[band]\n",
    "        rgb[:, :, plane] = (img * scale + m) * fI / I\n",
    "    rgb = torch.clip(rgb, 0, 1)\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def dr2_rgb(rimgs, bands, **ignored):\n",
    "    return sdss_rgb(\n",
    "        rimgs, bands, scales=dict(g=(2, 6.0), r=(1, 3.4), z=(0, 2.2)), m=0.03\n",
    "    )\n",
    "\n",
    "\n",
    "class toRGB(transforms.ToTensor):\n",
    "    def __init__(self, bands, scales=None, m=0.02):\n",
    "        self.bands = bands\n",
    "        self.scales = scales\n",
    "        self.m = m\n",
    "\n",
    "    def __call__(self, rimgs):\n",
    "        if len(rimgs.shape) == 3:\n",
    "            return dr2_rgb(rimgs.T, self.bands).T\n",
    "        if len(rimgs.shape) == 4:\n",
    "            img_outs = []\n",
    "            for img in rimgs:\n",
    "                img_outs.append(dr2_rgb(img.T, self.bands).T[None, :, :, :])\n",
    "            return torch.concatenate(img_outs)\n",
    "\n",
    "\n",
    "MEAN, STD = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "\n",
    "img_transforms = Compose(\n",
    "    [\n",
    "        Resize(152, InterpolationMode.BICUBIC),\n",
    "        ToTensor(),\n",
    "        CenterCrop(144),\n",
    "        Normalize(MEAN, STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "to_rgb = toRGB(bands=[\"g\", \"r\", \"z\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightning as L\n",
    "# from pl_bolts.models.self_supervised import Moco_v2\n",
    "\n",
    "# class OutputExtractor(L.LightningModule):\n",
    "#     \"\"\"\n",
    "#     Pass data through network to extract model outputs\n",
    "#     \"\"\"\n",
    "#     def __init__(self, backbone: torch.nn.Module):\n",
    "#         super(OutputExtractor, self).__init__()\n",
    "#         self.backbone = backbone\n",
    "#         self.backbone.eval()\n",
    "\n",
    "#     def forward(self, batch):\n",
    "#         x = batch\n",
    "#         z_emb = self.backbone(x)\n",
    "#         return z_emb\n",
    "\n",
    "#     def predict(self, batch, batch_idx: int, dataloader_idx: int=None):\n",
    "#         return self(batch)\n",
    "\n",
    "# # Extract encoder_q from Moco_v2 model\n",
    "# moco_model = Moco_v2.load_from_checkpoint(checkpoint_path='/mnt/ceph/users/flanusse/resnet50.ckpt')\n",
    "# backbone = moco_model.encoder_q\n",
    "# moco_model = OutputExtractor(backbone).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astroclip.cuda()\n",
    "DINO.cuda()\n",
    "# moco_model.cuda()\n",
    "CLIP_embeddings = []\n",
    "DINO_embeddings = []\n",
    "stein_embeddings = []\n",
    "\n",
    "batch, batch_size = [], 512\n",
    "total_batches = len(classifications) // batch_size\n",
    "\n",
    "for k, entry in enumerate(tqdm(classifications)):\n",
    "    image = torch.tensor(entry[\"image\"])\n",
    "    batch.append(image)\n",
    "\n",
    "    if len(batch) == batch_size:\n",
    "        batch = torch.stack(batch)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        # stein_embeddings.append(moco_model(batch.to(torch.float32).cuda()).detach().cpu().numpy())\n",
    "\n",
    "        batch = (\n",
    "            np.array(to_rgb(batch.permute(0, 2, 3, 1)) * 255)\n",
    "            .astype(\"uint8\")\n",
    "            .transpose(0, 2, 3, 1)\n",
    "        )\n",
    "        batch = torch.stack(\n",
    "            [img_transforms(im.fromarray(batch[i])) for i in range(batch.shape[0])]\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            CLIP_embeddings.append(\n",
    "                astroclip(batch.cuda(), image=True).detach().cpu().numpy()\n",
    "            )\n",
    "            DINO_embeddings.append(DINO.forward(batch.cuda()).detach().cpu().numpy())\n",
    "        batch = []\n",
    "\n",
    "    # do last batch\n",
    "    if k == len(classifications) - 1:\n",
    "        batch = torch.stack(batch)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     stein_embeddings.append(moco_model(batch.to(torch.float32).cuda()).detach().cpu().numpy())\n",
    "\n",
    "        batch = (\n",
    "            np.array(to_rgb(batch.permute(0, 2, 3, 1)) * 255)\n",
    "            .astype(\"uint8\")\n",
    "            .transpose(0, 2, 3, 1)\n",
    "        )\n",
    "        batch = torch.stack(\n",
    "            [img_transforms(im.fromarray(batch[i])) for i in range(batch.shape[0])]\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            CLIP_embeddings.append(\n",
    "                astroclip(batch.cuda(), image=True).detach().cpu().numpy()\n",
    "            )\n",
    "            DINO_embeddings.append(DINO.forward(batch.cuda()).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.softmax(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "def train_eval_MLP(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    embed_dim,\n",
    "    num_classes,\n",
    "    MLP_dim=128,\n",
    "    lr=1e-3,\n",
    "    epochs=25,\n",
    "    dropout=0.2,\n",
    "):\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    samples_weight = y_train.max(dim=1).values  # Taking max fraction as the weight\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=256,\n",
    "        sampler=WeightedRandomSampler(samples_weight, len(samples_weight)),\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    mlp = MLP(embed_dim, num_classes, MLP_dim, dropout)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Suitable for multi-label classification\n",
    "    optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_metrics = None\n",
    "\n",
    "    for epoch in range(epochs):  # Define your number of epochs\n",
    "        mlp.train()\n",
    "        train_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = mlp(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation loop\n",
    "        mlp.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = mlp(data)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Report every 25 epochs\n",
    "        # if epoch % 25 == 0:\n",
    "        #    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = mlp.state_dict()\n",
    "\n",
    "    mlp.load_state_dict(best_model)\n",
    "    y_pred = mlp(X_test).detach()\n",
    "\n",
    "    y_pred = (y_pred == torch.max(y_pred, dim=1, keepdim=True).values).int()\n",
    "    y_true = (y_test == torch.max(y_test, dim=1, keepdim=True).values).int()\n",
    "\n",
    "    accuracy = accuracy_score(y_true.numpy(), y_pred.numpy())\n",
    "    f1_score = precision_recall_fscore_support(\n",
    "        y_true.numpy(), y_pred.numpy(), average=\"weighted\", zero_division=0\n",
    "    )[2]\n",
    "    return {\"Accuracy\": accuracy, \"F1 Score\": f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_CLIP = torch.tensor(np.concatenate(CLIP_embeddings))\n",
    "X_DINO = torch.tensor(np.concatenate(DINO_embeddings))\n",
    "X_Stein = torch.tensor(np.concatenate(stein_embeddings))\n",
    "\n",
    "X = {}\n",
    "\n",
    "X[\"CLIP\"] = X_CLIP\n",
    "X[\"DINO\"] = X_DINO\n",
    "X[\"Stein\"] = X_Stein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/mnt/home/lparker/ceph/gz5_south/\"\n",
    "\n",
    "X = {}\n",
    "\n",
    "X[\"CLIP\"] = torch.load(dir + \"X_CLIP.pt\")\n",
    "X[\"DINO\"] = torch.load(dir + \"X_DINO.pt\")\n",
    "X[\"Stein\"] = torch.load(dir + \"X_Stein.pt\")\n",
    "\n",
    "classifications = Table.read(dir + \"classifications.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle all X and classifications in the same way\n",
    "np.random.seed(42)\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(classifications))\n",
    "\n",
    "for key in X.keys():\n",
    "    X[key] = X[key][shuffled_indices]\n",
    "\n",
    "classifications = classifications[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select first 80% for train and last 20% for test\n",
    "train_indices = int(0.8 * len(classifications))\n",
    "\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "\n",
    "for key in X.keys():\n",
    "    X_train[key] = X[key][:train_indices]\n",
    "    X_test[key] = X[key][train_indices:]\n",
    "\n",
    "classifications_train, classifications_test = (\n",
    "    classifications[:train_indices],\n",
    "    classifications[train_indices:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {}\n",
    "names = [\n",
    "    \"smooth\",\n",
    "    \"disk-edge-on\",\n",
    "    \"spiral-arms\",\n",
    "    \"bar\",\n",
    "    \"bulge-size\",\n",
    "    \"how-rounded\",\n",
    "    \"edge-on-bulge\",\n",
    "    \"spiral-winding\",\n",
    "    \"spiral-arm-count\",\n",
    "    \"merging\",\n",
    "]\n",
    "for name in names:\n",
    "    local_dict = {}\n",
    "    local_dict[\"debiased\"] = [\n",
    "        key for key in classifications.colnames if name in key and \"debiased\" in key\n",
    "    ]\n",
    "    local_dict[\"counts\"] = [\n",
    "        key for key in classifications.colnames if name in key and \"total-votes\" in key\n",
    "    ]\n",
    "    keys[name] = local_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts_train = classifications_train[keys[\"smooth\"][\"counts\"]].to_pandas().values\n",
    "\n",
    "outputs = {\"CLIP\": {}, \"DINO\": {}, \"Stein\": {}}\n",
    "\n",
    "for name in names:\n",
    "    question, num_classes = name, len(keys[name][\"debiased\"])\n",
    "\n",
    "    counts_train = classifications_train[keys[name][\"counts\"]].to_pandas().values\n",
    "    pct_answered = np.array(counts_train / total_counts_train)\n",
    "    above50 = np.where(pct_answered > 0.5)[0]\n",
    "\n",
    "    y_train = torch.tensor(\n",
    "        classifications_train[keys[name][\"debiased\"]].to_pandas().values\n",
    "    )[above50]\n",
    "    train_mask = torch.isnan(y_train).any(axis=1)\n",
    "    y_train = y_train[~train_mask]\n",
    "\n",
    "    counts_test = np.array(\n",
    "        classifications_test[keys[name][\"counts\"]].to_pandas().values\n",
    "    )\n",
    "    above35 = np.where(counts_test > 35)[0]\n",
    "\n",
    "    y_test = torch.tensor(\n",
    "        classifications_test[keys[name][\"debiased\"]].to_pandas().values\n",
    "    )[above35]\n",
    "    test_mask = torch.isnan(y_test).any(axis=1)\n",
    "    y_test = y_test[~test_mask]\n",
    "\n",
    "    categories = keys[name][\"debiased\"]\n",
    "    print(f\"Question: {question}, Classes: {categories}\")\n",
    "    print(f\"Number of classes: {num_classes}, Number of samples: {len(y_test)}\")\n",
    "\n",
    "    for model in X.keys():\n",
    "        X_train_local = X_train[model][above50][~train_mask]\n",
    "        X_test_local = X_test[model][above35][~test_mask]\n",
    "\n",
    "        outputs[model][name] = train_eval_MLP(\n",
    "            X_train_local,\n",
    "            X_test_local,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            X_train_local.shape[1],\n",
    "            num_classes=num_classes,\n",
    "            MLP_dim=128,\n",
    "            epochs=25,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        print(\n",
    "            f'{model} - Accuracy: {outputs[model][name][\"Accuracy\"]:.4f}, F1 Score: {outputs[model][name][\"F1 Score\"]:.4f}'\n",
    "        )\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_radar(\n",
    "    outputs, metric, file_path, title=\"Galaxy Property Estimation\", fontsize=25\n",
    "):\n",
    "    questions = {}\n",
    "    for key in outputs.keys():\n",
    "        questions[key] = [\n",
    "            outputs[key][question][metric] for question in outputs[key].keys()\n",
    "        ]\n",
    "\n",
    "    # Create radar chart\n",
    "    angles = np.linspace(0, 2 * pi, len(questions[key]), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # complete the loop\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(polar=True))\n",
    "\n",
    "    colors = [\"red\", \"red\", \"black\", \"blue\"]\n",
    "    styles = [\"solid\", \"dashed\", \"solid\", \"solid\"]\n",
    "\n",
    "    # Plot each array on the radar chart\n",
    "    for key in questions.keys():\n",
    "        # if key == 'ZooBot': continue\n",
    "        stats = [questions[key][i] for i in range(len(questions[key]))]\n",
    "        stats += stats[:1]\n",
    "        ax.plot(\n",
    "            angles,\n",
    "            stats,\n",
    "            label=key,\n",
    "            linewidth=2,\n",
    "            linestyle=styles.pop(0),\n",
    "            color=colors.pop(0),\n",
    "        )\n",
    "\n",
    "    labels = outputs[key].keys()\n",
    "\n",
    "    # capitalize labels\n",
    "    labels = [label.capitalize() for label in labels]\n",
    "\n",
    "    # Add labels with specific fontsize\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Change r label to fontsize\n",
    "    ax.tick_params(axis=\"y\", labelsize=fontsize)\n",
    "\n",
    "    ax.set_xticks(angles[:-1], labels, fontsize=fontsize, color=\"black\")\n",
    "\n",
    "    # make not overlap with plot\n",
    "    # ax.set_xticklabels(labels, fontsize=fontsize)\n",
    "\n",
    "    # make theta labels not overlap with plot\n",
    "    ax.set_ylim(0, 1.0)\n",
    "\n",
    "    # Add legend and title with specific fontsize\n",
    "    legend = plt.legend(loc=\"upper right\", bbox_to_anchor=(1.1, 1.1))\n",
    "    plt.setp(\n",
    "        legend.get_texts(), fontsize=fontsize\n",
    "    )  # Explicitly set fontsize for legend\n",
    "\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[\"ZooBot\"] = {\n",
    "    \"smooth\": {\"Accuracy\": 0.94, \"F1 Score\": 0.94},\n",
    "    \"disk-edge-on\": {\"Accuracy\": 0.99, \"F1 Score\": 0.99},\n",
    "    \"spiral-arms\": {\"Accuracy\": 0.93, \"F1 Score\": 0.94},\n",
    "    \"bar\": {\"Accuracy\": 0.82, \"F1 Score\": 0.81},\n",
    "    \"bulge-size\": {\"Accuracy\": 0.84, \"F1 Score\": 0.84},\n",
    "    \"how-rounded\": {\"Accuracy\": 0.93, \"F1 Score\": 0.93},\n",
    "    \"edge-on-bulge\": {\"Accuracy\": 0.91, \"F1 Score\": 0.90},\n",
    "    \"spiral-winding\": {\"Accuracy\": 0.78, \"F1 Score\": 0.79},\n",
    "    \"spiral-arm-count\": {\"Accuracy\": 0.77, \"F1 Score\": 0.76},\n",
    "    \"merging\": {\"Accuracy\": 0.88, \"F1 Score\": 0.85},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_radar(\n",
    "    outputs,\n",
    "    \"Accuracy\",\n",
    "    title=\"Galaxy Property Estimation\",\n",
    "    file_path=\"accuracy.png\",\n",
    "    fontsize=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_radar(\n",
    "    outputs,\n",
    "    \"F1 Score\",\n",
    "    title=\"Galaxy Property Estimation\",\n",
    "    file_path=\"f1_score.png\",\n",
    "    fontsize=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "acc, f1 = {}, {}\n",
    "for key in outputs.keys():\n",
    "    acc[key] = [outputs[key][question][\"Accuracy\"] for question in outputs[key].keys()]\n",
    "    f1[key] = [outputs[key][question][\"F1 Score\"] for question in outputs[key].keys()]\n",
    "\n",
    "acc = pd.DataFrame(acc, index=outputs[key].keys())\n",
    "f1 = pd.DataFrame(f1, index=outputs[key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save acc and f1\n",
    "acc.to_csv(\"accuracy.csv\")\n",
    "f1.to_csv(\"f1_score.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average over rows\n",
    "f1.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GZ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results for Smooth Question\n",
    "\n",
    "question, num_classes = \"Smooth\", 3\n",
    "\n",
    "smooth = torch.tensor(\n",
    "    classifications[\n",
    "        \"t01_smooth_or_features_a01_smooth_fraction\",\n",
    "        \"t01_smooth_or_features_a02_features_or_disk_fraction\",\n",
    "        \"t01_smooth_or_features_a03_star_or_artifact_fraction\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"CLIP\")\n",
    "outputs[\"CLIP\"][question] = train_eval_MLP(\n",
    "    X_CLIP, smooth, embed_dim=512, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"DINO\")\n",
    "outputs[\"DINO\"][question] = train_eval_MLP(\n",
    "    X_DINO, smooth, embed_dim=1024, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"Stein\")\n",
    "outputs[\"Stein\"][question] = train_eval_MLP(\n",
    "    X_Stein, smooth, embed_dim=128, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results for Edge On Question\n",
    "\n",
    "question, num_classes = \"Edge On\", 2\n",
    "\n",
    "counts = (\n",
    "    classifications[\"t02_edgeon_a04_yes_count\"]\n",
    "    + classifications[\"t02_edgeon_a05_no_count\"]\n",
    ")\n",
    "total = classifications[\"total_classifications\"]\n",
    "pct_answered = np.array(counts / total)\n",
    "above50 = np.where(pct_answered > 0.5)[0]\n",
    "\n",
    "y = classifications[\"t02_edgeon_a04_yes_fraction\", \"t02_edgeon_a05_no_fraction\"]\n",
    "y = torch.tensor(y[above50])\n",
    "\n",
    "print(\"CLIP\")\n",
    "outputs[\"CLIP\"][question] = train_eval_MLP(\n",
    "    X_CLIP[above50], y, embed_dim=512, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"DINO\")\n",
    "outputs[\"DINO\"][question] = train_eval_MLP(\n",
    "    X_DINO[above50], y, embed_dim=1024, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"Stein\")\n",
    "outputs[\"Stein\"][question] = train_eval_MLP(\n",
    "    X_Stein[above50], y, embed_dim=128, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results for Bar Question\n",
    "\n",
    "question, num_classes = \"Bar\", 2\n",
    "\n",
    "counts = (\n",
    "    classifications[\"t03_bar_a06_bar_count\"]\n",
    "    + classifications[\"t03_bar_a07_no_bar_count\"]\n",
    ")\n",
    "total = classifications[\"total_classifications\"]\n",
    "pct_answered = np.array(counts / total)\n",
    "above50 = np.where(pct_answered > 0.5)[0]\n",
    "\n",
    "y = classifications[\"t03_bar_a06_bar_fraction\", \"t03_bar_a07_no_bar_fraction\"]\n",
    "y = torch.tensor(y[above50])\n",
    "\n",
    "print(\"CLIP\")\n",
    "outputs[\"CLIP\"][question] = train_eval_MLP(\n",
    "    X_CLIP[above50], y, embed_dim=512, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"DINO\")\n",
    "outputs[\"DINO\"][question] = train_eval_MLP(\n",
    "    X_DINO[above50], y, embed_dim=1024, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"Stein\")\n",
    "outputs[\"Stein\"][question] = train_eval_MLP(\n",
    "    X_Stein[above50], y, embed_dim=128, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results for Spiral Question\n",
    "\n",
    "question, num_classes = \"Spiral Count\", 2\n",
    "\n",
    "counts = (\n",
    "    classifications[\"t04_spiral_a08_spiral_count\"]\n",
    "    + classifications[\"t04_spiral_a09_no_spiral_count\"]\n",
    ")\n",
    "total = classifications[\"total_classifications\"]\n",
    "pct_answered = np.array(counts / total)\n",
    "above50 = np.where(pct_answered > 0.5)[0]\n",
    "\n",
    "y = classifications[\n",
    "    \"t04_spiral_a08_spiral_fraction\", \"t04_spiral_a09_no_spiral_fraction\"\n",
    "]\n",
    "y = torch.tensor(y[above50])\n",
    "\n",
    "print(\"CLIP\")\n",
    "outputs[\"CLIP\"][question] = train_eval_MLP(\n",
    "    X_CLIP[above50], y, embed_dim=512, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"DINO\")\n",
    "outputs[\"DINO\"][question] = train_eval_MLP(\n",
    "    X_DINO[above50], y, embed_dim=1024, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"Stein\")\n",
    "outputs[\"Stein\"][question] = train_eval_MLP(\n",
    "    X_Stein[above50], y, embed_dim=128, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results for Bulge Prominence Question\n",
    "\n",
    "question, num_classes = \"Bulge Prominence\", 4\n",
    "\n",
    "counts = (\n",
    "    classifications[\"t05_bulge_prominence_a10_no_bulge_count\"]\n",
    "    + classifications[\"t05_bulge_prominence_a11_just_noticeable_count\"]\n",
    "    + classifications[\"t05_bulge_prominence_a12_obvious_count\"]\n",
    "    + classifications[\"t05_bulge_prominence_a13_dominant_count\"]\n",
    ")\n",
    "total = classifications[\"total_classifications\"]\n",
    "pct_answered = np.array(counts / total)\n",
    "above50 = np.where(pct_answered > 0.5)[0]\n",
    "\n",
    "y = classifications[\n",
    "    \"t05_bulge_prominence_a10_no_bulge_fraction\",\n",
    "    \"t05_bulge_prominence_a11_just_noticeable_fraction\",\n",
    "    \"t05_bulge_prominence_a12_obvious_fraction\",\n",
    "    \"t05_bulge_prominence_a13_dominant_fraction\",\n",
    "]\n",
    "y = torch.tensor(y[above50])\n",
    "\n",
    "print(\"CLIP\")\n",
    "outputs[\"CLIP\"][question] = train_eval_MLP(\n",
    "    X_CLIP[above50], y, embed_dim=512, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"DINO\")\n",
    "outputs[\"DINO\"][question] = train_eval_MLP(\n",
    "    X_DINO[above50], y, embed_dim=1024, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"Stein\")\n",
    "outputs[\"Stein\"][question] = train_eval_MLP(\n",
    "    X_Stein[above50], y, embed_dim=128, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results for How Rounded Question\n",
    "\n",
    "question, num_classes = \"How Rounded\", 3\n",
    "\n",
    "counts = (\n",
    "    classifications[\"t07_rounded_a16_completely_round_count\"]\n",
    "    + classifications[\"t07_rounded_a17_in_between_count\"]\n",
    "    + classifications[\"t07_rounded_a18_cigar_shaped_count\"]\n",
    ")\n",
    "total = classifications[\"total_classifications\"]\n",
    "pct_answered = np.array(counts / total)\n",
    "above50 = np.where(pct_answered > 0.5)[0]\n",
    "\n",
    "y = classifications[\n",
    "    \"t07_rounded_a16_completely_round_fraction\",\n",
    "    \"t07_rounded_a17_in_between_fraction\",\n",
    "    \"t07_rounded_a18_cigar_shaped_fraction\",\n",
    "]\n",
    "y = torch.tensor(y[above50])\n",
    "\n",
    "print(\"CLIP\")\n",
    "outputs[\"CLIP\"][question] = train_eval_MLP(\n",
    "    X_CLIP[above50], y, embed_dim=512, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"DINO\")\n",
    "outputs[\"DINO\"][question] = train_eval_MLP(\n",
    "    X_DINO[above50], y, embed_dim=1024, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"Stein\")\n",
    "outputs[\"Stein\"][question] = train_eval_MLP(\n",
    "    X_Stein[above50], y, embed_dim=128, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results for Bulge Shape Question\n",
    "\n",
    "question, num_classes = \"Bulge Shape\", 3\n",
    "\n",
    "counts = (\n",
    "    classifications[\"t09_bulge_shape_a25_rounded_count\"]\n",
    "    + classifications[\"t09_bulge_shape_a26_boxy_count\"]\n",
    "    + classifications[\"t09_bulge_shape_a27_no_bulge_count\"]\n",
    ")\n",
    "total = classifications[\"total_classifications\"]\n",
    "pct_answered = np.array(counts / total)\n",
    "above50 = np.where(pct_answered > 0.5)[0]\n",
    "\n",
    "y = classifications[\n",
    "    \"t07_rounded_a16_completely_round_fraction\",\n",
    "    \"t07_rounded_a17_in_between_fraction\",\n",
    "    \"t07_rounded_a18_cigar_shaped_fraction\",\n",
    "]\n",
    "y = torch.tensor(y[above50])\n",
    "\n",
    "print(\"CLIP\")\n",
    "outputs[\"CLIP\"][question] = train_eval_MLP(\n",
    "    X_CLIP[above50], y, embed_dim=512, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"DINO\")\n",
    "outputs[\"DINO\"][question] = train_eval_MLP(\n",
    "    X_DINO[above50], y, embed_dim=1024, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"Stein\")\n",
    "outputs[\"Stein\"][question] = train_eval_MLP(\n",
    "    X_Stein[above50], y, embed_dim=128, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results for Spiral Arm Type\n",
    "\n",
    "question, num_classes = \"Spiral Arm Type\", 3\n",
    "\n",
    "counts = (\n",
    "    classifications[\"t10_arms_winding_a28_tight_count\"]\n",
    "    + classifications[\"t10_arms_winding_a29_medium_count\"]\n",
    "    + classifications[\"t10_arms_winding_a30_loose_count\"]\n",
    ")\n",
    "total = classifications[\"total_classifications\"]\n",
    "pct_answered = np.array(counts / total)\n",
    "above50 = np.where(pct_answered > 0.5)[0]\n",
    "\n",
    "y = classifications[\n",
    "    \"t10_arms_winding_a28_tight_fraction\",\n",
    "    \"t10_arms_winding_a29_medium_fraction\",\n",
    "    \"t10_arms_winding_a30_loose_fraction\",\n",
    "]\n",
    "y = torch.tensor(y[above50])\n",
    "\n",
    "print(\"CLIP\")\n",
    "outputs[\"CLIP\"][question] = train_eval_MLP(\n",
    "    X_CLIP[above50], y, embed_dim=512, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"DINO\")\n",
    "outputs[\"DINO\"][question] = train_eval_MLP(\n",
    "    X_DINO[above50], y, embed_dim=1024, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"Stein\")\n",
    "outputs[\"Stein\"][question] = train_eval_MLP(\n",
    "    X_Stein[above50], y, embed_dim=128, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results for Spiral Arm Count\n",
    "\n",
    "question, num_classes = \"Spiral Arm Count\", 6\n",
    "\n",
    "counts = (\n",
    "    classifications[\"t11_arms_number_a31_1_count\"]\n",
    "    + classifications[\"t11_arms_number_a32_2_count\"]\n",
    "    + classifications[\"t11_arms_number_a33_3_count\"]\n",
    "    + classifications[\"t11_arms_number_a34_4_count\"]\n",
    "    + classifications[\"t11_arms_number_a36_more_than_4_count\"]\n",
    "    + classifications[\"t11_arms_number_a37_cant_tell_count\"]\n",
    ")\n",
    "total = classifications[\"total_classifications\"]\n",
    "pct_answered = np.array(counts / total)\n",
    "above50 = np.where(pct_answered > 0.5)[0]\n",
    "\n",
    "y = classifications[\n",
    "    \"t11_arms_number_a31_1_fraction\",\n",
    "    \"t11_arms_number_a32_2_fraction\",\n",
    "    \"t11_arms_number_a33_3_fraction\",\n",
    "    \"t11_arms_number_a34_4_fraction\",\n",
    "    \"t11_arms_number_a36_more_than_4_fraction\",\n",
    "    \"t11_arms_number_a37_cant_tell_fraction\",\n",
    "]\n",
    "y = torch.tensor(y[above50])\n",
    "\n",
    "print(\"CLIP\")\n",
    "outputs[\"CLIP\"][question] = train_eval_MLP(\n",
    "    X_CLIP[above50], y, embed_dim=512, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"DINO\")\n",
    "outputs[\"DINO\"][question] = train_eval_MLP(\n",
    "    X_DINO[above50], y, embed_dim=1024, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")\n",
    "\n",
    "print(\"Stein\")\n",
    "outputs[\"Stein\"][question] = train_eval_MLP(\n",
    "    X_Stein[above50], y, embed_dim=128, num_classes=num_classes, MLP_dim=32, epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_radar(\n",
    "    outputs, metric, file_path, title=\"Galaxy Property Estimation\", fontsize=22\n",
    "):\n",
    "    questions = {}\n",
    "    for key in outputs.keys():\n",
    "        questions[key] = [\n",
    "            outputs[key][question][metric] for question in outputs[key].keys()\n",
    "        ]\n",
    "\n",
    "    # Create radar chart\n",
    "    angles = np.linspace(0, 2 * pi, len(questions[key]), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # complete the loop\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(polar=True))\n",
    "\n",
    "    # Plot each array on the radar chart\n",
    "    for key in questions.keys():\n",
    "        stats = questions[key]\n",
    "        stats += stats[:1]\n",
    "        ax.plot(angles, stats, label=key)\n",
    "\n",
    "    labels = outputs[key].keys()\n",
    "\n",
    "    # Add labels with specific fontsize\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Change r label to fontsize\n",
    "    ax.tick_params(axis=\"y\", labelsize=fontsize)\n",
    "\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(\n",
    "        labels, fontsize=fontsize\n",
    "    )  # Explicitly set fontsize for xtick labels\n",
    "\n",
    "    # make theta labels not overlap with plot\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # Add legend and title with specific fontsize\n",
    "    legend = plt.legend(loc=\"upper right\", bbox_to_anchor=(1.1, 1.1))\n",
    "    plt.setp(\n",
    "        legend.get_texts(), fontsize=fontsize\n",
    "    )  # Explicitly set fontsize for legend\n",
    "\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_radar(\n",
    "    outputs,\n",
    "    \"Accuracy\",\n",
    "    title=\"Galaxy Property Estimation\",\n",
    "    file_path=\"accuracy.png\",\n",
    "    fontsize=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_radar(\n",
    "    outputs,\n",
    "    \"F1 Score\",\n",
    "    title=\"Galaxy Property Estimation\",\n",
    "    file_path=\"f1_score.png\",\n",
    "    fontsize=16,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astroclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
