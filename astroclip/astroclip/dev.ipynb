{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from astroclip.astroclip.preprocessing import ImageSpectrumCollator\n",
    "from astroclip.astroclip.data import AstroClipDataloader\n",
    "from astroclip.astroclip.modules import ImageModule, SpectrumModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = AstroClipDataloader(\n",
    "    \"/mnt/ceph/users/polymathic/mmoma/datasets/astroclip_file/\",\n",
    "    collate_fn=ImageSpectrumCollator(center_crop=144),\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "loader.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = next(iter(loader.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "imshow(dummy[\"image\"][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagemodule = ImageModule(\n",
    "    save_directory=\"/mnt/home/lparker/ceph/dino_training\",\n",
    "    config=\"/mnt/home/lparker/Documents/AstroFoundationModel/AstroDino_legacy/astrodino/configs/ssl_default_config.yaml\",\n",
    "    model_weights=\"/mnt/home/lparker/ceph/astrodino/vitl12_simplified_better_wd/training_199999/teacher_checkpoint.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagemodule.cuda()\n",
    "\n",
    "imagemodule.forward(dummy[\"image\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroclip.astroclip.modules import CrossAttentionHead, MLP\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "\n",
    "class SpectrumModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: str,\n",
    "        model_weights: str,\n",
    "        embed_dim: int = 1024,\n",
    "        n_head: int = 4,\n",
    "        model_embed_dim: int = 768,\n",
    "        dropout: float = 0.1,\n",
    "        freeze_backbone: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Cross-attention spectrum module that takes a spectrum and passes it through a pretrained SpecFormer model and\n",
    "        then through a cross-attention mechanism and MLP to get the final embedding.\n",
    "\n",
    "        Args:\n",
    "            save_path (str): Path to the checkpoint of the SpecFormer model.\n",
    "            embed_dim (int): Dimension of the AstroCLIP embedding.\n",
    "            n_head (int): Number of heads in the multihead attention.\n",
    "            model_embed_dim (int): Dimension of the SpecFormer embedding.\n",
    "            dropout (float): Dropout rate for MLP layers.\n",
    "            freeze_backbone (bool): Whether to freeze the backbone of the SpecFormer model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Load the model from the checkpoint\n",
    "        checkpoint = torch.load(model_weights)\n",
    "        config = yaml.safe_load(open(config))\n",
    "\n",
    "        self.backbone.load_from_checkpoint(checkpoint[\"state_dict\"])\n",
    "\n",
    "        # Freeze backbone if necessary\n",
    "        self.freeze_backbone = freeze_backbone\n",
    "        if self.freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Set up cross-attention\n",
    "        self.cross_attention = CrossAttentionHead(\n",
    "            embed_dim=embed_dim,\n",
    "            n_head=n_head,\n",
    "            model_embed_dim=model_embed_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Set up MLP\n",
    "        self.mlp = MLP(\n",
    "            embed_dim=embed_dim,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.tensor, y: torch.tensor = None, return_weights: bool = False\n",
    "    ):\n",
    "        # Slice the spectrum\n",
    "        # TODO: use spectrum collate function\n",
    "        x = fnc(x.unsqueeze(-1))\n",
    "\n",
    "        # Embed the spectrum using the pretrained model\n",
    "        if self.freeze_backbone:\n",
    "            with torch.no_grad():\n",
    "                embedding = self.backbone(x)[\"embedding\"]\n",
    "        else:\n",
    "            embedding = self.backbone(x)[\"embedding\"]\n",
    "\n",
    "        # Pass through cross-attention\n",
    "        x, attentions = self.cross_attention(embedding)\n",
    "\n",
    "        # Pass through MLP and residual connection\n",
    "        x += self.mlp(x)\n",
    "\n",
    "        if return_weights:\n",
    "            return x.squeeze(), attentions[1]\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def load_config(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "load_config(\n",
    "    \"/mnt/home/lparker/Documents/AstroFoundationModel/AstroCLIP/astroclip/specformer/config.yaml\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
