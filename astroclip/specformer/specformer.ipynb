{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "\n",
    "import astroclip\n",
    "from astroclip import format_with_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = load_from_disk(format_with_env(\"{ASTROCLIP_ROOT}/datasets/astroclip_file/\"))\n",
    "dataset.set_format(type=\"torch\", columns=[\"spectrum\"])\n",
    "\n",
    "loader = {\n",
    "    k: DataLoader(dataset[k], batch_size=128, shuffle=True) for k in [\"train\", \"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the samples\n",
    "# This step, Z-scores each sample individually and encodes\n",
    "# this information is stored in the first element of each sample\n",
    "# Then the sample is then unfolded into overlapping regions\n",
    "# as a continuous tokenization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def slice(x, section_length=10, overlap=5):\n",
    "    start_indices = np.arange(0, len(x) - overlap, section_length - overlap)\n",
    "    sections = [x[start : start + section_length] for start in start_indices]\n",
    "\n",
    "    # If the last section is not of length 'section_length', you can decide whether to keep or discard it\n",
    "    if len(sections[-1]) < section_length:\n",
    "        sections.pop(-1)  # Discard the last section\n",
    "\n",
    "    return np.concatenate(sections, 1).T\n",
    "\n",
    "\n",
    "def preprocess(samples):\n",
    "    out = []\n",
    "\n",
    "    for x in samples[\"spectrum\"]:\n",
    "        x = np.array(x)\n",
    "        std, mean = x.std(), x.mean()\n",
    "        # skipping samples that are all zero\n",
    "        if std == 0:\n",
    "            continue\n",
    "        x = (x - mean) / std\n",
    "        x = slice(x, 194, 97)\n",
    "        x = np.pad(x, pad_width=((1, 0), (2, 0)), mode=\"constant\", constant_values=0)\n",
    "\n",
    "        x[0, 0] = (mean - 2) / 2\n",
    "        x[0, 1] = (std - 2) / 8\n",
    "\n",
    "        out.append(x)\n",
    "    # print(len(out))\n",
    "    return {\"spectrum\": torch.tensor(out)}\n",
    "\n",
    "\n",
    "# for training we drop chunks of the spectrum\n",
    "def drop_chunks(batch, size=5):\n",
    "    batch[\"input\"] = batch[\"spectrum\"].clone()\n",
    "    # random start location between 0 and length of the spectrum\n",
    "    start1 = torch.randint(0, batch[\"spectrum\"].shape[1] - 3 * size - 2, (1,)).item()\n",
    "    start2 = torch.randint(\n",
    "        start1 + size + 1, batch[\"spectrum\"].shape[1] - 2 * size - 1, (1,)\n",
    "    ).item()\n",
    "    start3 = torch.randint(\n",
    "        start2 + size + 1, batch[\"spectrum\"].shape[1] - size, (1,)\n",
    "    ).item()\n",
    "    batch[\"input\"][:, start1 : start1 + size] *= 0\n",
    "    batch[\"input\"][:, start2 : start2 + size] *= 0\n",
    "    batch[\"input\"][:, start3 : start3 + size] *= 0\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader[\"train\"]))\n",
    "\n",
    "batch_train = drop_chunks(preprocess(batch))\n",
    "sp_ = batch_train[\"spectrum\"][3, :, 4]\n",
    "in_ = batch_train[\"input\"][3, :, 4]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(in_, label=\"dropped\")\n",
    "plt.plot(sp_, label=\"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpecConfig:\n",
    "    input_dim: int\n",
    "    embed_dim: int\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    max_len: int\n",
    "    dropout: float = 0.1\n",
    "    norm_first: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SpecFormer(nn.Module):\n",
    "    config: SpecConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.embed = nn.Linear(config.input_dim, config.embed_dim)\n",
    "\n",
    "        self.head = nn.Linear(config.embed_dim, 1)\n",
    "\n",
    "        self.abs_pos = nn.Embedding(config.max_len, config.embed_dim)\n",
    "\n",
    "        trans_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.embed_dim,\n",
    "            nhead=config.num_heads,\n",
    "            dim_feedforward=4 * config.embed_dim,\n",
    "            dropout=config.dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=config.norm_first,\n",
    "        )\n",
    "\n",
    "        self.encoder_stack = nn.TransformerEncoder(\n",
    "            encoder_layer=trans_layer,\n",
    "            num_layers=config.num_layers,\n",
    "            enable_nested_tensor=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        t = input.shape[1]\n",
    "\n",
    "        if len(input.shape) == 2:\n",
    "            input = input.unsqueeze(-1)\n",
    "\n",
    "        x = self.embed(input) + self.abs_pos.weight[:t].unsqueeze(0)\n",
    "\n",
    "        x = F.gelu(self.encoder_stack(x))\n",
    "\n",
    "        # adding the input back in so we model the difference\n",
    "        x = input + self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = SpecConfig(\n",
    "    input_dim=196,\n",
    "    embed_dim=192,\n",
    "    num_layers=3,\n",
    "    num_heads=3,\n",
    "    max_len=80,\n",
    "    dropout=0.1,\n",
    "    norm_first=False,\n",
    ")\n",
    "\n",
    "my_model = SpecFormer(my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "def train_model(model, loader, num_epochs, lr, weight_decay):\n",
    "    model.cuda()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.AdamW(my_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # add cosine annealing scheduler\n",
    "    sched = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, len(loader) * num_epochs, lr / 100\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            # Set the model to train mode\n",
    "            model.train()\n",
    "\n",
    "            # Initialize the total loss for this epoch\n",
    "            total_loss = 0\n",
    "\n",
    "            # Iterate over the training data\n",
    "            for batch in tqdm(loader):\n",
    "                # preprocess the batch\n",
    "                batch = drop_chunks(preprocess(batch))\n",
    "\n",
    "                # Clear the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(batch[\"input\"].cuda())\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = loss_fn(output, batch[\"spectrum\"].cuda())\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "\n",
    "                sched.step()\n",
    "\n",
    "                # Update the total loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Compute the average loss for this epoch\n",
    "            avg_loss = total_loss / len(loader)\n",
    "\n",
    "            # Print the average loss for this epoch\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.3g}, Learning Rate: {optimizer.param_groups[0]['lr']:.4f}\"\n",
    "            )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"KeyboardInterrupt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(my_model, loader[\"train\"], num_epochs=10, lr=1e-4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\"model_state_dict\": my_model.state_dict(), \"config\": my_config},\n",
    "    \"trained_model.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "\n",
    "checkpoint = torch.load(\"trained_model.pth\")\n",
    "my_model = SpecFormer(checkpoint[\"config\"])\n",
    "my_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "my_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = 2\n",
    "\n",
    "batch = next(iter(loader[\"train\"]))\n",
    "batch_train = drop_chunks(preprocess(batch))\n",
    "out = my_model(batch_train[\"input\"].cuda()).cpu()\n",
    "\n",
    "sp_ = batch_train[\"spectrum\"][samp, :, 6]\n",
    "in_ = batch_train[\"input\"][samp, :, 6]\n",
    "\n",
    "plt.plot(in_, label=\"dropped\")\n",
    "plt.plot(sp_, label=\"original\")\n",
    "plt.plot(out[samp, :, 4].detach(), label=\"reconstructed\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = batch[\"spectrum\"].shape[0]\n",
    "sp_rec = batch_train[\"spectrum\"][:, 1:, 2:99].reshape(bs, -1)[0]\n",
    "in_rec = batch_train[\"input\"][:, 1:, 2:99].reshape(bs, -1)[0]\n",
    "out_rec = out[:, 1:, 2:99].reshape(bs, -1)[0]\n",
    "\n",
    "plt.plot(in_rec, label=\"dropped\")\n",
    "plt.plot(sp_rec, label=\"original\")\n",
    "plt.plot(out_rec.detach(), label=\"reconstructed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 3\n",
    "\n",
    "bs = batch[\"spectrum\"].shape[0]\n",
    "sp_rec = batch_train[\"spectrum\"][:, 1:, 2:99].reshape(bs, -1)[sample]\n",
    "in_rec = batch_train[\"input\"][:, 1:, 2:99].reshape(bs, -1)[sample]\n",
    "out_rec = out[:, 1:, 2:99].reshape(bs, -1)[sample]\n",
    "\n",
    "# plot the moving average of the spectrum\n",
    "win = 20\n",
    "\n",
    "sp_rec = [sp_rec[i : i + 20].mean().item() for i in range(0, len(sp_rec), 20)]\n",
    "in_rec = [in_rec[i : i + 20].mean().item() for i in range(0, len(in_rec), 20)]\n",
    "out_rec = [out_rec[i : i + 20].mean().item() for i in range(0, len(out_rec), 20)]\n",
    "\n",
    "plt.plot(in_rec, label=\"dropped\")\n",
    "plt.plot(sp_rec, label=\"original\")\n",
    "plt.plot(out_rec, label=\"reconstructed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 4\n",
    "\n",
    "bs = batch[\"spectrum\"].shape[0]\n",
    "sp_rec = batch_train[\"spectrum\"][:, 1:, 2:99].mean(-1)[sample]\n",
    "in_rec = batch_train[\"input\"][:, 1:, 2:99].mean(-1)[sample]\n",
    "out_rec = out[:, 1:, 2:99].mean(-1)[sample]\n",
    "\n",
    "plt.plot(in_rec, label=\"dropped\")\n",
    "plt.plot(sp_rec, label=\"original\")\n",
    "plt.plot(out_rec.detach(), label=\"reconstructed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astroclip_fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
