{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EiffL/Tutorials/blob/master/FoundationModels/AstroCLIPTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2024-2025 Francois Lanusse.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ],
      "metadata": {
        "id": "YLY8S9K38yl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AstroCLIP Tutorial: Galaxy Similarity Search and Redshift Prediction\n",
        "\n",
        "Author: [@EiffL](https://github.com/EiffL) (Francois Lanusse)\n",
        "\n",
        "\n",
        "In this notebook, we will explore how to use **AstroCLIP**, a cross-modal foundation model trained to embed galaxy images and spectra into a shared, physically meaningful space.\n",
        "\n",
        "We'll use AstroCLIP to perform two tasks:\n",
        "\n",
        "- **Similarity Search**: retrieve galaxies that are similar to a given image or spectrum, even across modalities.\n",
        "- **Redshift Prediction**: train a lightweight prediction model on top of AstroCLIP embeddings to estimate photometric redshifts.\n",
        "\n",
        "AstroCLIP was trained in a self-supervised way on millions of galaxies from the Legacy Survey, enabling powerful transfer learning without requiring fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Learning Objectives\n",
        "\n",
        "In this notebook, you will learn how to:\n",
        "\n",
        "- Use a pre-trained cross-modal embedding model for scientific downstream tasks  \n",
        "- Perform **semantic search** using cosine similarity  \n",
        "- Train a simple redshift predictor on top of **frozen AstroCLIP embeddings**\n",
        "\n",
        "---\n",
        "\n",
        "### ⚙️ Instructions for Enabling GPU Access\n",
        "\n",
        "By default, notebooks are started without acceleration. To make sure that the runtime is configured for using GPUs, go to `Runtime > Change runtime type`, and select GPU in `Hardware Accelerator`.\n",
        "\n",
        "---\n",
        "\n",
        "### Installing dependencies\n",
        "\n",
        "The install procedure is unfortunately a bit complex, execute the following cell. **Do not restart kernel when prompted**, instead run the cell below."
      ],
      "metadata": {
        "id": "rv-9tGWx9aof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base install of required utilities\n",
        "!apt install python3-dev libcairo2-dev pkg-config\n",
        "\n",
        "# Setting up proper torch version\n",
        "!pip install --upgrade pip\n",
        "!pip install lightning[extra]==2.3.3 boto3==1.28.17\n",
        "!pip install --upgrade pycairo datasets pyarrow\n",
        "!pip install --extra-index-url https://pypi.nvidia.com cuml-cu11\n",
        "!pip install --extra-index-url https://download.pytorch.org/whl/cu117 torch==2.0.0+cu117\n",
        "!pip install torchvision==0.15.0 torchmetrics==0.10.3 dotenv\n",
        "!pip install numpy==1.26.4 --force-reinstall\n",
        "\n",
        "# Installing DiNOv2\n",
        "!pip install omegaconf fvcore iopath\n",
        "!pip install --no-deps git+https://github.com/facebookresearch/dinov2.git@2302b6bf46953431b969155307b9bed152754069\n",
        "\n",
        "# Installing AstroCLIP\n",
        "!pip install astropy datasets huggingface_hub jaxtyping wandb networkx pyvis\n",
        "!pip uninstall -y transformers\n",
        "!pip install --no-deps git+https://github.com/PolymathicAI/AstroCLIP.git"
      ],
      "metadata": {
        "id": "Jz92vu0OCKtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restarting kernel after everything is installed\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "f80KChdwM1O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading model weights\n",
        "!wget -O astroclip.ckpt https://huggingface.co/polymathic-ai/astroclip/resolve/main/astroclip.ckpt?download=true"
      ],
      "metadata": {
        "id": "rVaaEPBUi0Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, run the following cell to setup some utility and plotting functions"
      ],
      "metadata": {
        "id": "8UolL-ZcfjyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [Run me]\n",
        "%pylab inline\n",
        "import pyarrow\n",
        "pyarrow.PyExtensionType.set_auto_load(True)\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "from pyvis.network import Network\n",
        "from IPython.display import HTML\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.preprocessing import normalize\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from ipywidgets import interact\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "def graph_from_embeddings(embeddings, images, k=5, symmetrize=True):\n",
        "    \"\"\"\n",
        "    Builds a k-NN graph adjacency matrix using scikit-learn (cosine distance).\n",
        "\n",
        "    Returns a sparse adjacency matrix (numpy array or CSR).\n",
        "    \"\"\"\n",
        "    # Convert to NumPy and normalize (cosine similarity = dot of L2-normalized vectors)\n",
        "    emb_np = embeddings.cpu().numpy()\n",
        "\n",
        "    # scikit-learn uses cosine *distance*, so we negate similarity in behavior\n",
        "    A = kneighbors_graph(emb_np, n_neighbors=k, metric='cosine', mode='connectivity', include_self=False)\n",
        "\n",
        "    if symmetrize:\n",
        "        A = A.maximum(A.T)  # make it symmetric (undirected)\n",
        "\n",
        "    G = nx.Graph()\n",
        "    A = A.tocoo()\n",
        "\n",
        "    for i in range(A.shape[0]):\n",
        "        G.add_node(i, image_tensor=images[i].permute(1, 2, 0).cpu().numpy() if images is not None else None)\n",
        "\n",
        "    for i, j, w in zip(A.row, A.col, A.data):\n",
        "        G.add_edge(i, j, weight=w)\n",
        "\n",
        "    return G\n",
        "\n",
        "def encode_image_base64(image_array, scale=1.):\n",
        "    fig = plt.figure(figsize=(scale, scale), dpi=100)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(image_array)\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)\n",
        "    buf.seek(0)\n",
        "    img_base64 = base64.b64encode(buf.read()).decode('utf-8')\n",
        "    return f\"data:image/png;base64,{img_base64}\"\n",
        "\n",
        "def draw_interactive_graph_colab(graph):\n",
        "    net = Network(height='750px', width='100%', bgcolor='#000000', font_color='white', notebook=False, cdn_resources='remote')\n",
        "\n",
        "    # Encode each image as base64 and use as node icon\n",
        "    for node_id, data in graph.nodes(data=True):\n",
        "        img_url = encode_image_base64(data['image_tensor'])\n",
        "        net.add_node(\n",
        "            int(node_id),\n",
        "            shape=\"image\",\n",
        "            image=img_url,\n",
        "            title=f\"Galaxy {node_id}\",\n",
        "            size=50\n",
        "        )\n",
        "\n",
        "    for src, tgt, data in graph.edges(data=True):\n",
        "        net.add_edge(int(src), int(tgt), value=float(data['weight']) * 5,\n",
        "                     color='rgba(255, 255, 255, 0.5)')\n",
        "\n",
        "    # Generate the HTML string (without writing to file)\n",
        "    html_str = net.generate_html()\n",
        "\n",
        "    # Display directly in Colab\n",
        "    return HTML(html_str)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vVDaDIopcJlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing data\n",
        "\n",
        "For this tutorial, we are going to use an already cross-matched compilation of images and spectra, from the Legacy Surveys and DESI respectively.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kgx9Yd7rP6vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from astroclip.data.datamodule import AstroClipCollator"
      ],
      "metadata": {
        "id": "Z2Ki3A70ZX2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads cross-matched data between DESI and Legacy Survey images\n",
        "dset = load_dataset('EiffL/AstroCLIP', streaming=True, split='train')\n",
        "dset = dset.with_format('torch')\n",
        "\n",
        "# Creates a torch data loader for the data\n",
        "dloader = DataLoader(dset, batch_size=256,\n",
        "                      collate_fn=AstroClipCollator(), drop_last=True)\n",
        "iter_dset = iter(dloader)"
      ],
      "metadata": {
        "id": "f02pX7VvRCQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can extract one bach of objects like so:\n",
        "batch = next(iter_dset)\n",
        "\n",
        "print(batch.keys())"
      ],
      "metadata": {
        "id": "827Cnl4hRCMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that a batch of object contains an image, a spectrum, a redshift, and a targetid.\n",
        "\n",
        "Let's visualize a few images:"
      ],
      "metadata": {
        "id": "lwTAxGdPSHuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = batch['image']\n",
        "\n",
        "# Assuming images is a tensor of shape (batch_size, channels, height, width)\n",
        "# and we want to plot the first 64 images in an 8x8 grid.\n",
        "fig, axes = subplots(8, 8, figsize=(10, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(min(len(images), 64)):\n",
        "    img = images[i].permute(1, 2, 0).numpy()  # Convert to HWC for matplotlib\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "tight_layout()"
      ],
      "metadata": {
        "id": "bTlzyagVRr8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And we can also look at the redshifts we get in this batch\n",
        "z = batch['redshift']\n",
        "hist(z, bins=32)\n",
        "xlabel('Redshift');"
      ],
      "metadata": {
        "id": "f0jOi__URr5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing embeddings with AstroCLIP\n",
        "\n",
        "Now that we have seen how to load our data, let's see how to apply the AstroCLIP model."
      ],
      "metadata": {
        "id": "9toDfwlpTLuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from astroclip.models import AstroClipModel\n",
        "\n",
        "# Loads the model from downloaded checkpoint\n",
        "model = AstroClipModel.load_from_checkpoint(\n",
        "    checkpoint_path = \"astroclip.ckpt\",\n",
        ").eval()"
      ],
      "metadata": {
        "id": "KipbrBOujf3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  # Apply the model on images\n",
        "  embeddings = model(batch['image'].to('cuda'),\n",
        "              input_type='image') # AstroCLIP understands 'image' or 'spectum' as input_type"
      ],
      "metadata": {
        "id": "8ru8yR1-KQRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings)\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "GEruVJD6Thqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that our AstroCLIP model has returned a set of 256 embeddings of size 1024.\n",
        "\n",
        "\n",
        "Let's look at the structure of that embedding space by adjacency in terms of cosine similarity:"
      ],
      "metadata": {
        "id": "bCTvVHUVTncT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = graph_from_embeddings(embeddings, images=images, k=2)\n",
        "draw_interactive_graph_colab(G)"
      ],
      "metadata": {
        "id": "9O2dQn8ijUFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's compute systematically embeddings for a large sample of objects."
      ],
      "metadata": {
        "id": "xcUyFmrGBGQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "redshift = []\n",
        "images = []\n",
        "\n",
        "for i in tqdm(range(20)):\n",
        "  batch = next(iter_dset)\n",
        "  with torch.no_grad():\n",
        "    emb = model(batch['image'].to('cuda'),\n",
        "              input_type='image').to('cpu')\n",
        "\n",
        "    images.append(batch['image'].numpy())\n",
        "    redshift.append(batch['redshift'].numpy())\n",
        "    embeddings.append(emb.numpy())\n",
        "\n",
        "embeddings = np.concatenate(embeddings)\n",
        "redshift = np.concatenate(redshift)\n",
        "images = np.concatenate(images)"
      ],
      "metadata": {
        "id": "d3Gg3LhyBFYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "vlaq5MlAGIve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔎 Similarity Search in Embedding Space\n",
        "\n",
        "In this section, we perform a **similarity search** to find galaxies that are most similar to a selected query image, based on their positions in the learned **embedding space**.\n",
        "\n",
        "We use **cosine similarity** to measure how close each image embedding is to the embedding of the selected query image. This allows us to identify objects that share similar high-level visual or morphological features, as captured by the embedding model.\n",
        "\n",
        "The steps are:\n",
        "- Select a query galaxy image.\n",
        "- Compute cosine similarity between its embedding and all others in the dataset.\n",
        "- Retrieve and display the **top 16 most similar images** (excluding the query itself).\n",
        "\n",
        "This technique enables **fast, content-based retrieval** from large image datasets, and can be a powerful tool for exploratory data analysis and serendipitous discovery.\n"
      ],
      "metadata": {
        "id": "iIwZZZ6hRlnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_images = images[:10]\n",
        "selected_index = {'value': 0}  # Use a mutable dict\n",
        "\n",
        "# Define the display function\n",
        "def show_query_image(index):\n",
        "    selected_index['value'] = index  # Store the selection\n",
        "    img = query_images[index]\n",
        "    # Convert CHW to HWC for matplotlib\n",
        "    img_hwc = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(img_hwc)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Selected Query Image: {index}')\n",
        "    plt.show()\n",
        "\n",
        "# Use interact to create a dropdown or slider\n",
        "interact(show_query_image, index=(0, len(query_images) - 1));"
      ],
      "metadata": {
        "id": "i_xqteQeG7c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query_index = selected_index['value']\n",
        "query_embedding = embeddings[query_index].reshape(1, -1)\n",
        "\n",
        "# TODO: compute the cosine similarity between query embedding and emebeddings\n",
        "similarity = #... ADD CODE HERE\n",
        "similarity = similarity.squeeze()"
      ],
      "metadata": {
        "id": "kTBvw5eDHg7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get indices of all embeddings\n",
        "all_indices = np.arange(len(embeddings))\n",
        "# Sort by similarity in descending order\n",
        "sorted_indices = all_indices[np.argsort(similarity)[::-1]]\n",
        "\n",
        "# Exclude the query index if it's among the top results\n",
        "# We want the top 16 *other* objects\n",
        "top_n = 16\n",
        "top_indices = sorted_indices[:top_n]\n",
        "\n",
        "# Retrieve the images corresponding to these top indices\n",
        "top_images = images[top_indices]\n",
        "\n",
        "# Draw the postage stamps\n",
        "fig, axes = subplots(4, 4, figsize=(8, 8)) # Create a 4x4 grid for 16 images\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(min(len(top_images), 16)):\n",
        "    img = top_images[i]\n",
        "    # Convert CHW to HWC for matplotlib\n",
        "    img_hwc = np.transpose(img, (1, 2, 0))\n",
        "    axes[i].imshow(img_hwc)\n",
        "    axes[i].axis('off')\n",
        "    # Optional: Add title indicating the index of the retrieved image in the original dataset\n",
        "    axes[i].set_title(f'Obj {top_indices[i]}', fontsize=8)\n",
        "\n",
        "tight_layout()\n",
        "plt.suptitle(f'Top {top_n} Most Similar Objects to Obj {query_index}', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rAW1RY9CISR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📈 Redshift Estimation with k-Nearest Neighbors (k-NN) Regression\n",
        "\n",
        "In this section, we use a **k-Nearest Neighbors (k-NN) regressor** to predict galaxy redshifts directly from their image embeddings.\n",
        "\n",
        "The idea is simple: for a given embedding, find its `k` closest neighbors in the embedding space and return the **average redshift** of those neighbors. This method relies on the assumption that similar embeddings (i.e., similar galaxies) should have similar redshifts.\n",
        "\n",
        "Here’s what we do:\n",
        "- Split the dataset into training and testing sets.\n",
        "- Train a `KNeighborsRegressor` using the training embeddings and redshifts.\n",
        "- Predict redshifts for the test set.\n",
        "- Visualize the performance by plotting **true vs. predicted redshifts**.\n",
        "\n",
        "This provides a quick baseline and helps assess how well the learned embedding space captures redshift-related information.\n"
      ],
      "metadata": {
        "id": "OJb-ynmBR4SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error"
      ],
      "metadata": {
        "id": "HA34wDDjMsir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for k-NN regression\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, redshift, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "gAuypP0VNMZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: create a KNeighborsRegressor with n_neighbors 10\n",
        "# and fit it to the train data\n",
        "# See documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
        "\n",
        "knn_regressor = #... ADD CODE HERE\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict redshift on the test set\n",
        "y_pred = knn_regressor.predict(X_test)\n",
        "\n",
        "# Plot true vs predicted redshift (optional)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "plt.xlabel(\"True Redshift\")\n",
        "plt.ylabel(\"Predicted Redshift\")\n",
        "plt.title(f\"True vs. Predicted Redshift\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Add a diagonal line\n",
        "plt.xlim(0,0.5)\n",
        "plt.ylim(0,0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zl9nRDzkMpMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔍 Inferring Redshifts with Implicit Inference\n",
        "\n",
        "In this section, we use Implicit Inference to model the conditional distribution of galaxy redshift given an image embedding. Specifically, we apply the **Sequential Neural Posterior Estimation (SNPE-A)** algorithm from the `sbi` library.\n",
        "\n",
        "Rather than predicting a single redshift value, this approach learns the full **posterior distribution** \\( p(\\text{redshift} \\mid \\text{embedding}) \\), capturing uncertainty and potential ambiguities in the mapping from image features to physical properties.\n",
        "\n",
        "Here's what we do:\n",
        "- Treat our training set of embeddings (`X_train`) and corresponding redshifts (`y_train`) as observed simulations.\n",
        "- Define a simple prior over redshift values (uniform between 0 and 1).\n",
        "- Use SNPE-A to train a neural density estimator that approximates the posterior over redshift given a new embedding.\n",
        "\n",
        "This allows us to move beyond point estimates and toward **probabilistic reasoning**, providing more informative outputs for downstream scientific analysis.\n"
      ],
      "metadata": {
        "id": "VUoru6LUQGwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sbi"
      ],
      "metadata": {
        "id": "SLEmnbNzOscc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sbi.inference import SNPE_A\n",
        "from sbi.utils import BoxUniform\n",
        "from torch import Tensor\n",
        "\n",
        "# Setting up the training data\n",
        "x_obs = torch.tensor(X_train, dtype=torch.float32)\n",
        "theta_obs = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1) # sbi expects parameter tensors to have shape (num_samples, num_parameters)\n",
        "\n",
        "# Define a simple prior for the redshift. Assuming redshifts are between 0 and 1 for simplicity.\n",
        "# In a real scenario, you'd define a prior that reflects your knowledge about the redshift distribution.\n",
        "prior = BoxUniform(low=torch.zeros(1), high=torch.ones(1))\n",
        "\n",
        "# Building the inference method\n",
        "inference = SNPE_A(prior=prior)\n",
        "inference.append_simulations(theta_obs, x_obs)\n",
        "\n",
        "# Train the posterior estimator\n",
        "# This step trains the neural network to learn p(theta | x) based on the provided data.\n",
        "print(\"Training the SBI posterior estimator...\")\n",
        "density_estimator = inference.train()\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "_0RnPuTlM_-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sbi.analysis import pairplot\n",
        "\n",
        "# Now we can use the trained density estimator to estimate the posterior distribution of redshift\n",
        "# for new observations (embeddings) in the test set X_test.\n",
        "\n",
        "# Convert X_test to tensor\n",
        "x_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Estimate the posterior distribution for the first test sample\n",
        "# You can loop through the test set or process in batches\n",
        "print(\"\\nEstimating posterior for a test sample...\")\n",
        "# Let's pick the first sample from the test set\n",
        "test_sample_embedding = x_test_tensor[0].unsqueeze(0) # Add batch dimension\n",
        "\n",
        "# Get the posterior object for this observation\n",
        "posterior = inference.build_posterior(density_estimator=density_estimator)\n",
        "\n",
        "# Sample from the posterior distribution for this observation\n",
        "num_samples = 1000\n",
        "posterior_samples = posterior.sample(sample_shape=(num_samples,), x=test_sample_embedding)\n",
        "\n",
        "print(f\"True redshift for this sample: {y_test[0]:.4f}\")\n",
        "# Calculate the mean of the posterior samples as a point estimate\n",
        "predicted_redshift_mean = posterior_samples.mean().item()\n",
        "print(f\"Posterior mean redshift estimate: {predicted_redshift_mean:.4f}\")\n",
        "\n",
        "# Let's plot the posterior for the first test sample again\n",
        "fig, axes = pairplot(posterior_samples, limits=[[0, 1]], labels=['Redshift'], figsize=(5,5))\n",
        "plt.suptitle(f\"Posterior distribution for Test Sample 0 (True Z: {y_test[0]:.4f})\")\n",
        "\n",
        "# Overlay the true redshift as a vertical line\n",
        "true_z = y_test[0]\n",
        "axes.axvline(true_z, color='red', linestyle='--', label=f'True Z = {true_z:.4f}')\n",
        "axes.legend()\n",
        "\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "TBWl2gUrPgJJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOuzflhboaEERM+SY5WaAqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}